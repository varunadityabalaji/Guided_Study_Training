{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a23f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pandas_ta as ta\n",
    "import http.client, urllib.parse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import *\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib \n",
    "from flask import jsonify\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_data(ticker_symbol,df):\n",
    "    dates = df['Date']\n",
    "    dates = dates.apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\"))\n",
    "    sentiment_values = []\n",
    "\n",
    "    for m in dates:        \n",
    "        #the request is put in a try catch block to stop the loop from breaking\n",
    "        try:\n",
    "            #initiate the connection to the 3rd party api service\n",
    "            conn = http.client.HTTPSConnection('api.marketaux.com') \n",
    "            params = urllib.parse.urlencode({\n",
    "                'api_token': 'fkkywOjEYioULrZrV9pt21k6pTtRPW5C17FeWNkE',\n",
    "                'symbols': ticker_symbol,\n",
    "                'published_on':m\n",
    "                })\n",
    "\n",
    "            conn.request('GET', '/v1/entity/stats/aggregation?{}'.format(params))\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            parsed = json.loads(data)\n",
    "            sentiment_values.append(parsed['data'][0]['sentiment_avg'])\n",
    "            print(m)\n",
    "        except Exception as e:\n",
    "            return 'error'      \n",
    "        time.sleep(1)\n",
    "        \n",
    "    return sentiment_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b643ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ticker):\n",
    "    indicators = ['High','Low','Open','Volume','Adj Close','H-L','O-C','5MA',\n",
    "              '10MA','20MA','7SD','EMA8','EMA21','EMA34','EMA55','RSI_14','Sentiment']\n",
    "    \n",
    "    end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.today() - relativedelta(months=+4)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        df = web.DataReader(name=ticker, data_source='yahoo', start=start_date, end=end_date)\n",
    "    except Exception as e:\n",
    "        return 'Please check ticker symbol'\n",
    "    \n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df['Close'] = df['Close'].shift(-1)\n",
    "    df = df.reset_index()\n",
    "    sentiment_values =get_sentiment_data(ticker,df)\n",
    "    \n",
    "    if sentiment_values == 'error':\n",
    "        response = {'status':'failure','message':'problem with 3rd party api'}\n",
    "        return response\n",
    "    \n",
    "    df['Sentiment'] = sentiment_values\n",
    "    today_data = df.iloc[-5:-1][indicators]\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    X = np.asarray(df[indicators], np.float32)\n",
    "    Y = np.asarray(df['Close'], np.float32)\n",
    "    today_data = np.asarray(today_data,np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    mms_X = MinMaxScaler()\n",
    "    mms_y = MinMaxScaler()\n",
    "\n",
    "    scaler_x = mms_X.fit(X_train)\n",
    "    scaler_y = mms_y.fit(y_train)\n",
    "\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train)\n",
    "\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "    y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    pca = PCA(n_components=4)\n",
    "\n",
    "    X_train= pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "    # Defining ANN neural network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, input_shape=(1,4), activation='relu'))\n",
    "    model.add(Dense(units=50,activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Fitting the ANN to the Training set\n",
    "    history = model.fit(X_train, y_train ,batch_size = 10, validation_data = (X_test, y_test), epochs = 15, verbose=1)\n",
    "\n",
    "    scaled_data = scaler_x.transform(today_data)\n",
    "    scaled_data = pca.fit_transform(scaled_data)\n",
    "    scaled_data = np.expand_dims(scaled_data, axis=1)\n",
    "    \n",
    "    pred = model.predict(scaled_data)\n",
    "    pred = scaler_y.inverse_transform(pred.reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8082a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17\n",
      "2022-03-18\n",
      "2022-03-21\n",
      "2022-03-22\n",
      "2022-03-23\n",
      "2022-03-24\n",
      "2022-03-25\n",
      "2022-03-28\n",
      "2022-03-29\n",
      "2022-03-30\n",
      "2022-03-31\n",
      "2022-04-01\n",
      "2022-04-04\n",
      "2022-04-05\n",
      "2022-04-06\n",
      "2022-04-07\n",
      "2022-04-08\n",
      "2022-04-11\n",
      "2022-04-12\n",
      "2022-04-13\n",
      "2022-04-14\n",
      "2022-04-18\n",
      "2022-04-19\n",
      "2022-04-20\n",
      "2022-04-21\n",
      "2022-04-22\n",
      "2022-04-25\n",
      "2022-04-26\n",
      "2022-04-27\n",
      "2022-04-28\n",
      "2022-04-29\n",
      "2022-05-02\n",
      "2022-05-03\n",
      "2022-05-04\n",
      "2022-05-05\n",
      "2022-05-06\n",
      "2022-05-09\n",
      "2022-05-10\n",
      "2022-05-11\n",
      "2022-05-12\n",
      "2022-05-13\n",
      "2022-05-16\n",
      "2022-05-17\n",
      "2022-05-18\n",
      "2022-05-19\n",
      "2022-05-20\n",
      "2022-05-23\n",
      "2022-05-24\n",
      "2022-05-25\n",
      "2022-05-26\n",
      "2022-05-27\n",
      "2022-05-31\n",
      "2022-06-01\n",
      "2022-06-02\n",
      "2022-06-03\n",
      "2022-06-06\n",
      "2022-06-07\n",
      "2022-06-08\n",
      "2022-06-09\n",
      "2022-06-10\n",
      "2022-06-13\n",
      "2022-06-14\n",
      "2022-06-15\n",
      "2022-06-16\n",
      "2022-06-17\n",
      "2022-06-21\n",
      "2022-06-22\n",
      "2022-06-23\n",
      "2022-06-24\n",
      "2022-06-27\n",
      "2022-06-28\n",
      "2022-06-29\n",
      "2022-06-30\n",
      "2022-07-01\n",
      "2022-07-05\n",
      "2022-07-06\n",
      "2022-07-07\n",
      "2022-07-08\n",
      "2022-07-11\n",
      "2022-07-12\n",
      "2022-07-13\n",
      "2022-07-14\n",
      "2022-07-15\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 01:20:21.307434: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 73ms/step - loss: 0.0761 - val_loss: 0.0349\n",
      "Epoch 2/15\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.0766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 01:20:21.741072: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0729 - val_loss: 0.0330\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0696 - val_loss: 0.0310\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0661 - val_loss: 0.0287\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0620 - val_loss: 0.0265\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0579 - val_loss: 0.0241\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0536 - val_loss: 0.0218\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0492 - val_loss: 0.0194\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0441 - val_loss: 0.0173\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0390 - val_loss: 0.0156\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0343 - val_loss: 0.0141\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0303 - val_loss: 0.0129\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0259 - val_loss: 0.0122\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0220 - val_loss: 0.0119\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0194 - val_loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 01:20:23.135731: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 490ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[144.61021],\n",
       "       [144.53032],\n",
       "       [149.86627],\n",
       "       [151.39723]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92005dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8bd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_data = apple.iloc[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f718e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ce7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['open'] = apple.iloc[-1]['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0894767e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.77999877929688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2a3ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172.19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['openig'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f59650dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"89\":174.9199981689,\"90\":172.0,\"91\":172.1699981689,\"92\":172.1900024414,\"93\":175.0800018311,\"94\":175.5299987793,\"95\":172.1900024414}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['h']=df.iloc[-8:-1]['Close'].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eafa358",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsonify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjsonify\u001b[49m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonify' is not defined"
     ]
    }
   ],
   "source": [
    "jsonify(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e68bb82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[174.9199981689453, 172.0, 172.1699981689453, 172.19000244140625, 175.0800018310547, 175.52999877929688, 172.19000244140625]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(a.reshape(7).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fa9fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153ae8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
